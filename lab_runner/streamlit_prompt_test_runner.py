import streamlit as st
import json
import os
from typing import Dict, Any, List
from dataclasses import dataclass
from pathlib import Path
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv
import logging
import time
import datetime
import csv
import pandas as pd

# åŠ è½½ç¯å¢ƒå˜é‡å¹¶è®¾ç½®æ—¥å¿—
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    name: str
    input_data: Dict[str, Any]
    expected_output: Dict[str, Any]

@dataclass
class TestResult:
    test_case: TestCase
    actual_output: Dict[str, Any]
    passed: bool
    error_message: str = ""
    execution_time: float = 0.0

class PromptTestRunner:
    def __init__(self):
        load_dotenv()
        
        # å®šä¹‰æç¤ºè¯ç›®å½•
        self.prompt_dir = "prompt_engineering/bot194/01"
        
        # å®šä¹‰æ—¥å¿—ç›®å½•å’Œæ–‡ä»¶
        self.log_dir = Path(__file__).parent / "test_logs"
        self.log_dir.mkdir(exist_ok=True)
        self.csv_log_file = self.log_dir / "test_results.csv"
        self.detail_log_dir = self.log_dir / "details"
        self.detail_log_dir.mkdir(exist_ok=True)
        
        # å¦‚æœCSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
        if not self.csv_log_file.exists():
            with open(self.csv_log_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'æµ‹è¯•æ—¶é—´',
                    'æç¤ºè¯ç³»ç»Ÿ',
                    'æ¨¡å‹',
                    'æµ‹è¯•ç”¨ä¾‹',
                    'æ‰§è¡Œæ—¶é—´(ç§’)',
                    'æµ‹è¯•ç»“æœ',
                    'é”™è¯¯ä¿¡æ¯',
                    'è¯¦ç»†æ—¥å¿—æ–‡ä»¶'
                ])
        
        # å®šä¹‰æç¤ºè¯é…ç½®æ˜ å°„
        self.prompt_configs = {
            "å»ºé€ ç³»ç»Ÿ": {
                "prompt": "bot_skill_build_prompt_02.md",
                "test_cases": "bot_skill_build_test_cases.md"
            },
            "èµ„æºç®¡ç†": {
                "prompt": "resource_manager_prompt_02.md",
                "test_cases": "resource_manager_test_cases.md"
            },
            "æ•°ç»„å®šä¹‰": {
                "prompt": "array_definition_prompt.md",
                "test_cases": "array_definition_test_cases.md"
            },
        }

    def load_prompt(self, prompt_name: str):
        """åŠ è½½é€‰å®šçš„æç¤ºè¯"""
        config = self.prompt_configs[prompt_name]
        self.prompt_filename = config["prompt"]
        self.test_cases_filename = config["test_cases"]
        
        # ä»æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤ºè¯
        prompt_file = Path(__file__).parent.parent / self.prompt_dir / self.prompt_filename
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # å¤„ç†æç¤ºè¯ä¸­çš„å˜é‡æ ‡è®°
        content = content.replace("{", "{{").replace("}", "}}")
        content = content.replace("{{{{", "{").replace("}}}}", "}")

        # ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
        self.system_prompt = content + """
æ³¨æ„ï¼š
1. ä½ å¿…é¡»ç›´æ¥è¿”å›JSONæ ¼å¼æ•°æ®ï¼Œä¸è¦åœ¨JSONå‰åæ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ã€å¯¹è¯æˆ–è¯´æ˜
2. è¿”å›çš„JSONå¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´ä¸”æœ‰æ•ˆçš„JSONå¯¹è±¡
3. æ‰€æœ‰å­—æ®µå€¼å¿…é¡»ç¬¦åˆå…¶é¢„æœŸçš„æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚ï¼šæ•°å­—ã€å­—ç¬¦ä¸²ã€å¸ƒå°”å€¼ã€å¯¹è±¡ç­‰ï¼‰
4. ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨äºè¾“å‡ºä¸­
5. ä¸è¦åœ¨JSONä¸­æ·»åŠ æ³¨é‡Š
6. è¾“å‡ºJSONçš„é”®å€¼ç»Ÿä¸€ä½¿ç”¨å°å†™å­—æ¯
"""
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt)
        ])
        return len(self.system_prompt)

    def load_test_cases(self, test_file_path: str) -> List[TestCase]:
        """ä»Markdownæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []
        
        with open(test_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        def extract_json_content(text: str, marker: str) -> str:
            start_marker = f"##### {marker}\n```json"
            end_marker = "```"
            start = text.find(start_marker)
            if start == -1:
                return ""
            start = start + len(start_marker)
            end = text.find(end_marker, start)
            if end == -1:
                return ""
            json_text = text[start:end].strip()
            return json_text
        
        test_sections = content.split('#### æµ‹è¯•ç”¨ä¾‹')
        loaded_cases = []
        errors = []
        
        for section in test_sections[1:]:
            try:
                name = section.split('\n')[0].strip()
                input_text = extract_json_content(section, "input.json")
                output_text = extract_json_content(section, "output.json")
                
                if not input_text or not output_text:
                    errors.append(f"æµ‹è¯•ç”¨ä¾‹ {name} ç¼ºå°‘è¾“å…¥æˆ–è¾“å‡ºJSON")
                    continue
                
                try:
                    input_json = json.loads(input_text)
                    output_json = json.loads(output_text)
                    
                    test_cases.append(TestCase(
                        name=name,
                        input_data=input_json,
                        expected_output=output_json
                    ))
                    loaded_cases.append(name)
                except json.JSONDecodeError as je:
                    errors.append(f"JSONè§£æé”™è¯¯ - æµ‹è¯•ç”¨ä¾‹ {name}: {str(je)}")
                    continue
                    
            except Exception as e:
                errors.append(f"è§£æé”™è¯¯ - æµ‹è¯•ç”¨ä¾‹ {name if 'name' in locals() else 'unknown'}: {str(e)}")
                continue
        
        return test_cases, loaded_cases, errors

    def _compare_outputs(self, actual: Dict[str, Any], expected: Dict[str, Any]) -> tuple[bool, list]:
        """æ¯”è¾ƒå®é™…è¾“å‡ºå’Œé¢„æœŸè¾“å‡ºï¼Œè¿”å›æ˜¯å¦åŒ¹é…å’Œä¸åŒ¹é…çš„è¯¦ç»†ä¿¡æ¯"""
        def compare_dicts(actual_dict: Dict[str, Any], expected_dict: Dict[str, Any], path: str = "") -> tuple[bool, list]:
            errors = []
            for key, expected_value in expected_dict.items():
                if key not in actual_dict:
                    errors.append(f"ç¼ºå°‘å­—æ®µ {path}{key}")
                    continue
                    
                actual_value = actual_dict[key]
                
                if isinstance(expected_value, dict):
                    if not isinstance(actual_value, dict):
                        errors.append(f"å­—æ®µç±»å‹ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› dictï¼Œå®é™… {type(actual_value).__name__}")
                        continue
                    sub_result, sub_errors = compare_dicts(actual_value, expected_value, f"{path}{key}.")
                    errors.extend(sub_errors)
                elif isinstance(expected_value, (int, float)):
                    if not isinstance(actual_value, (int, float)):
                        errors.append(f"æ•°å€¼ç±»å‹ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› numberï¼Œå®é™… {type(actual_value).__name__}")
                        continue
                    if abs(actual_value - expected_value) > 0.01:
                        errors.append(f"æ•°å€¼ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› {expected_value}ï¼Œå®é™… {actual_value}")
                elif isinstance(expected_value, bool):
                    if not isinstance(actual_value, bool):
                        errors.append(f"å¸ƒå°”ç±»å‹ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› boolï¼Œå®é™… {type(actual_value).__name__}")
                        continue
                    if actual_value != expected_value:
                        errors.append(f"å¸ƒå°”å€¼ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› {expected_value}ï¼Œå®é™… {actual_value}")
                elif isinstance(expected_value, str):
                    if not isinstance(actual_value, str):
                        errors.append(f"å­—æ®µç±»å‹ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› stringï¼Œå®é™… {type(actual_value).__name__}")
                        continue
                    # å­—ç¬¦ä¸²åªæ£€æŸ¥ç±»å‹ï¼Œä¸æ¯”è¾ƒå†…å®¹
                elif isinstance(expected_value, list):
                    if not isinstance(actual_value, list):
                        errors.append(f"åˆ—è¡¨ç±»å‹ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› listï¼Œå®é™… {type(actual_value).__name__}")
                        continue
                    if len(actual_value) != len(expected_value):
                        errors.append(f"åˆ—è¡¨é•¿åº¦ä¸åŒ¹é… {path}{key}ï¼šæœŸæœ› {len(expected_value)}ï¼Œå®é™… {len(actual_value)}")
                    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ åˆ—è¡¨å…ƒç´ çš„è¯¦ç»†æ¯”è¾ƒ
                    
            return len(errors) == 0, errors
            
        is_match, error_details = compare_dicts(actual, expected)
        return is_match, error_details

    def run_test(self, test_case: Dict[str, Any], expected_output: Dict[str, Any], model_name: str) -> tuple[bool, Dict[str, Any], float, str]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹å¹¶è¿”å›ç»“æœã€å®é™…è¾“å‡ºã€æ‰§è¡Œæ—¶é—´å’Œé”™è¯¯ä¿¡æ¯"""
        start_time = time.time()
        error_msg = ""
        actual_output = {}
        
        try:
            input_json = json.dumps(test_case, ensure_ascii=False, indent=2)
            
            system_content = self.system_prompt.format(
                input=test_case.get("input", ""),
                context=json.dumps(test_case.get("context", {}), ensure_ascii=False)
            )
            
            try:
                chat = ChatOpenAI(
                    model=model_name,
                    temperature=float(os.getenv("TEMPERATURE", "0.7")),
                    base_url=os.getenv("OPENAI_API_BASE"),
                    api_key=os.getenv("OPENAI_API_KEY"),
                    streaming=True
                )
                
                messages = [
                    SystemMessage(content=system_content),
                    HumanMessage(content=input_json)
                ]
                
                result = chat.invoke(messages)
                
                try:
                    content = result.content
                    json_start = content.find('{')
                    json_end = content.rfind('}')
                    if json_start != -1 and json_end != -1:
                        content = content[json_start:json_end + 1]
                    
                    actual_output = json.loads(content)
                    self._validate_output_format(actual_output)
                    
                    is_match, error_details = self._compare_outputs(actual_output, expected_output)
                    if is_match:
                        return True, actual_output, time.time() - start_time, ""
                    else:
                        error_msg = "è¾“å‡ºä¸é¢„æœŸä¸åŒ¹é…ï¼š\n" + "\n".join(error_details)
                        return False, actual_output, time.time() - start_time, error_msg
                        
                except json.JSONDecodeError:
                    error_msg = "AIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼"
                    return False, {}, time.time() - start_time, error_msg
                    
            except Exception as e:
                error_msg = f"APIè°ƒç”¨é”™è¯¯: {str(e)}"
                return False, {}, time.time() - start_time, error_msg
                
        except Exception as e:
            error_msg = f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {str(e)}"
            return False, {}, time.time() - start_time, error_msg

    def _validate_output_format(self, output: Dict[str, Any]):
        """éªŒè¯è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆè§„èŒƒ"""
        if not isinstance(output, dict):
            raise ValueError("Output must be a dictionary")
        
        required_fields = ["updated_context", "process", "botstatus", "message", "dialogue"]
        for field in required_fields:
            if field not in output:
                raise ValueError(f"Missing required field: {field}")

    def save_test_results(self, prompt_system: str, model: str, case_name: str, 
                         result: Dict[str, Any], test_time: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°CSVå’Œè¯¦ç»†æ—¥å¿—æ–‡ä»¶"""
        if test_time is None:
            test_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
        # åˆ›å»ºè¯¦ç»†æ—¥å¿—æ–‡ä»¶å
        detail_file_name = f"{test_time.replace(':', '-')}_{model}_{case_name}.json"
        detail_file_path = self.detail_log_dir / detail_file_name
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
        with open(detail_file_path, 'w', encoding='utf-8') as f:
            json.dump({
                "test_time": test_time,
                "prompt_system": prompt_system,
                "model": model,
                "case_name": case_name,
                "input_data": result["input_data"],
                "expected_output": result["expected_output"],
                "actual_output": result["actual_output"],
                "passed": result["passed"],
                "execution_time": result["execution_time"],
                "error": result["error"]
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶
        with open(self.csv_log_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                test_time,
                prompt_system,
                model,
                case_name,
                f"{result['execution_time']:.2f}",
                "é€šè¿‡" if result["passed"] else "å¤±è´¥",
                result["error"] or "",
                detail_file_name
            ])

def main():
    st.set_page_config(
        page_title="Prompt Test Runner",
        page_icon="ğŸ§ª",
        layout="wide"
    )
    
    st.title("ğŸ§ª Prompt Test Runner")
    
    # åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
    if 'runner' not in st.session_state:
        st.session_state.runner = PromptTestRunner()
        st.session_state.test_cases = []
        st.session_state.loaded_cases = []
        st.session_state.errors = []
        st.session_state.results = {}
    
    # å¯é€‰æ¨¡å‹åˆ—è¡¨
    models = [
        "moonshot-v1-32k",
        "Doubao-pro-128k",
        "gpt-3.5-turbo",
        "claude-3-sonnet-20240229",
        "claude-3-5-sonnet-20240620",
        "claude-3-5-sonnet-20241022",
        "c-3-5-sonnet-20241022",
        "gpt-4-turbo",
        "qwen-max",
        "glm-4"
    ]

    # baseline æ¨¡å‹åˆ—è¡¨
    baseline_models = [
        "moonshot-v1-32k",
#        "Doubao-pro-128k",
        "claude-3-5-sonnet-20241022"
#        "gpt-4-turbo",
#        "glm-4"
    ]
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("é…ç½®")
        
        # é€‰æ‹©æç¤ºè¯ç³»ç»Ÿ
        prompt_system = st.selectbox(
            "é€‰æ‹©æç¤ºè¯ç³»ç»Ÿ",
            options=list(st.session_state.runner.prompt_configs.keys())
        )
        
        # é€‰æ‹©æ¨¡å‹æ¨¡å¼
        model_mode = st.radio(
            "é€‰æ‹©æ¨¡å‹æ¨¡å¼",
            options=["å•ä¸ªæ¨¡å‹", "Baselineæ¨¡å‹ç»„"],
            index=0
        )

        if model_mode == "å•ä¸ªæ¨¡å‹":
            # é€‰æ‹©å•ä¸ªæ¨¡å‹
            selected_model = st.selectbox(
                "é€‰æ‹©æµ‹è¯•æ¨¡å‹",
                options=models
            )
            selected_models = [selected_model]
        else:
            # ä½¿ç”¨baselineæ¨¡å‹ç»„
            selected_models = baseline_models
            st.info("å°†ä½¿ç”¨ä»¥ä¸‹baselineæ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼š\n" + "\n".join([f"- {model}" for model in baseline_models]))
    
    # ä¸»ç•Œé¢
    if st.button("åŠ è½½æç¤ºè¯å’Œæµ‹è¯•ç”¨ä¾‹", key="load_button"):
        with st.spinner("æ­£åœ¨åŠ è½½..."):
            # åŠ è½½æç¤ºè¯
            prompt_length = st.session_state.runner.load_prompt(prompt_system)
            st.success(f"æç¤ºè¯åŠ è½½å®Œæˆï¼Œé•¿åº¦ï¼š{prompt_length}")
            
            # åŠ è½½æµ‹è¯•ç”¨ä¾‹
            test_file = Path(__file__).parent.parent / st.session_state.runner.prompt_dir / st.session_state.runner.test_cases_filename
            st.session_state.test_cases, st.session_state.loaded_cases, st.session_state.errors = st.session_state.runner.load_test_cases(str(test_file))
            
            if st.session_state.test_cases:
                st.success(f"æˆåŠŸåŠ è½½ {len(st.session_state.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            if st.session_state.errors:
                st.error("\n".join(st.session_state.errors))
    
    # ä¸»ç•Œé¢
    if st.session_state.test_cases:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("æµ‹è¯•ç”¨ä¾‹")
            
            # åˆå§‹åŒ–é€‰ä¸­çš„æµ‹è¯•ç”¨ä¾‹
            if 'selected_test_cases' not in st.session_state:
                st.session_state.selected_test_cases = [st.session_state.test_cases[0].name]
            
            # é€‰æ‹©å…¨éƒ¨æŒ‰é’®
            if st.button("é€‰æ‹©å…¨éƒ¨ç”¨ä¾‹"):
                st.session_state.selected_test_cases = [case.name for case in st.session_state.test_cases]
            
            # æµ‹è¯•ç”¨ä¾‹å¤šé€‰æ¡†
            selected_cases = st.multiselect(
                "é€‰æ‹©è¦è¿è¡Œçš„æµ‹è¯•ç”¨ä¾‹",
                options=[case.name for case in st.session_state.test_cases],
                default=st.session_state.selected_test_cases
            )
            # æ›´æ–°é€‰ä¸­çš„æµ‹è¯•ç”¨ä¾‹
            st.session_state.selected_test_cases = selected_cases
        
        with col2:
            st.subheader("è¿è¡Œæµ‹è¯•")
            if st.button("è¿è¡Œæµ‹è¯•"):
                st.session_state.results = {}
                
                # å¯¹æ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹è¿è¡Œæµ‹è¯•
                test_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                for model in selected_models:
                    st.subheader(f"æ¨¡å‹: {model}")
                    model_results = {}
                    
                    for case_name in selected_cases:
                        test_case = next(case for case in st.session_state.test_cases if case.name == case_name)
                        with st.spinner(f"æ­£åœ¨è¿è¡Œæµ‹è¯•ç”¨ä¾‹: {case_name}"):
                            passed, actual_output, execution_time, error = st.session_state.runner.run_test(
                                test_case.input_data,
                                test_case.expected_output,
                                model
                            )
                            
                            result = {
                                "passed": passed,
                                "execution_time": execution_time,
                                "error": error,
                                "actual_output": actual_output,
                                "expected_output": test_case.expected_output,
                                "input_data": test_case.input_data
                            }
                            
                            model_results[case_name] = result
                            
                            # ä¿å­˜æµ‹è¯•ç»“æœ
                            st.session_state.runner.save_test_results(
                                prompt_system,
                                model,
                                case_name,
                                result,
                                test_time
                            )
                    
                st.session_state.results[model] = model_results
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
        if st.session_state.results:
            st.subheader("æµ‹è¯•ç»“æœ")
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹æ˜¾ç¤ºç»“æœ
            for model, model_results in st.session_state.results.items():
                st.markdown(f"### æ¨¡å‹: {model}")
                
                # è®¡ç®—ç»Ÿè®¡æ•°æ®
                total = len(model_results)
                passed = sum(1 for r in model_results.values() if r["passed"])
                failed = total - passed
                pass_rate = (passed/total)*100 if total > 0 else 0
                total_time = sum(r["execution_time"] for r in model_results.values())
                avg_time = total_time / total if total > 0 else 0
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                cols = st.columns(6)
                cols[0].metric("æ€»ç”¨ä¾‹æ•°", total)
                cols[1].metric("é€šè¿‡", passed)
                cols[2].metric("å¤±è´¥", failed)
                cols[3].metric("é€šè¿‡ç‡", f"{pass_rate:.1f}%")
                cols[4].metric("æ€»è€—æ—¶", f"{total_time:.2f}ç§’")
                cols[5].metric("å¹³å‡è€—æ—¶", f"{avg_time:.2f}ç§’")
                
                # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                for case_name, result in model_results.items():
                    with st.expander(f"{'âœ…' if result['passed'] else 'âŒ'} {case_name} ({result['execution_time']:.2f}ç§’)"):
                        if result["error"]:
                            st.error(result["error"])
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.subheader("è¾“å…¥")
                            st.json(result["input_data"])
                        
                        with col2:
                            st.subheader("æœŸæœ›è¾“å‡º")
                            st.json(result["expected_output"])
                        
                        with col3:
                            st.subheader("å®é™…è¾“å‡º")
                            if result["actual_output"]:
                                st.json(result["actual_output"])
                            else:
                                st.error("æ— è¾“å‡º")

        # æ·»åŠ æŸ¥çœ‹å†å²è®°å½•éƒ¨åˆ†
        st.divider()  # æ·»åŠ åˆ†éš”çº¿
        st.subheader("å†å²è®°å½•æŸ¥çœ‹")
        
        if os.path.exists(st.session_state.runner.csv_log_file):
            # ä¸‹è½½æŒ‰é’®å’Œæœ€è¿‘è®°å½•æ˜¾ç¤º
            with open(st.session_state.runner.csv_log_file, 'r', encoding='utf-8') as f:
                csv_content = f.read()
            st.download_button(
                "ä¸‹è½½CSVæµ‹è¯•è®°å½•",
                csv_content,
                "test_results.csv",
                "text/csv",
                key='download_csv'
            )
            
            # æ˜¾ç¤ºæœ€è¿‘çš„æµ‹è¯•è®°å½•
            st.subheader("æœ€è¿‘çš„æµ‹è¯•è®°å½•")
            df = pd.read_csv(st.session_state.runner.csv_log_file)
            st.dataframe(df.tail(10))
            
            # ä¸‰æ å¸ƒå±€æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            st.subheader("è¯¦ç»†æ—¥å¿—æŸ¥çœ‹")
            log_files = list(st.session_state.runner.detail_log_dir.glob("*.json"))
            if log_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
                log_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
                
                # åˆ›å»ºä¸€ä¸ªæ›´å‹å¥½çš„æ˜¾ç¤ºæ ¼å¼
                log_options = {}
                for f in log_files:
                    # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    mtime = datetime.datetime.fromtimestamp(os.path.getmtime(f))
                    # åˆ›å»ºæ˜¾ç¤ºåç§°ï¼šæ—¶é—´ - æ¨¡å‹ - æµ‹è¯•ç”¨ä¾‹
                    display_name = f"{mtime.strftime('%Y-%m-%d %H:%M:%S')} - {f.stem}"
                    log_options[display_name] = f
                
                # ä¸Šéƒ¨åˆ†ï¼šä¸¤æ å¸ƒå±€æ˜¾ç¤ºæ—¥å¿—åˆ—è¡¨å’ŒåŸºæœ¬ä¿¡æ¯
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("#### æ—¥å¿—åˆ—è¡¨")
                    selected_log_display = st.selectbox(
                        "é€‰æ‹©è¦æŸ¥çœ‹çš„æ—¥å¿—",
                        options=list(log_options.keys()),
                        key='log_selector_new'
                    )
                
                if selected_log_display:
                    selected_log_file = log_options[selected_log_display]
                    with open(selected_log_file, 'r', encoding='utf-8') as f:
                        log_data = json.load(f)
                        
                    with col2:
                        st.markdown("#### æµ‹è¯•ä¿¡æ¯")
                        st.write(f"**æµ‹è¯•æ—¶é—´:** {log_data['test_time']}")
                        st.write(f"**æç¤ºè¯ç³»ç»Ÿ:** {log_data['prompt_system']}")
                        st.write(f"**æ¨¡å‹:** {log_data['model']}")
                        st.write(f"**æµ‹è¯•ç”¨ä¾‹:** {log_data['case_name']}")
                        st.write(f"**æ‰§è¡Œæ—¶é—´:** {log_data['execution_time']:.2f}ç§’")
                        st.write(f"**æµ‹è¯•ç»“æœ:** {'âœ… é€šè¿‡' if log_data['passed'] else 'âŒ å¤±è´¥'}")
                        if log_data['error']:
                            st.error(f"é”™è¯¯ä¿¡æ¯:\n{log_data['error']}")
                    
                    # ä¸‹éƒ¨åˆ†ï¼šä¸‰æ å¸ƒå±€æ˜¾ç¤ºæ•°æ®
                    st.divider()
                    data_col1, data_col2, data_col3 = st.columns(3)
                    
                    with data_col1:
                        st.markdown("#### è¾“å…¥æ•°æ®")
                        st.json(log_data['input_data'])
                    
                    with data_col2:
                        st.markdown("#### æœŸæœ›è¾“å‡º")
                        st.json(log_data['expected_output'])
                        
                    with data_col3:
                        st.markdown("#### å®é™…è¾“å‡º")
                        st.json(log_data['actual_output'])

if __name__ == "__main__":
    main()
